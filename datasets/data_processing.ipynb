{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# import torch\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "import torch\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "import json\n",
    "import chardet\n",
    "from transformers import BertConfig,BertTokenizer,BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datas_type=[]\n",
    "datas=[]\n",
    "file=open(\"/kaggle/input/fewfc/train_label.json\", \"rb\")\n",
    "for line in file:\n",
    "    datas.append(json.loads(line))\n",
    "file.close()\n",
    "texts=[]\n",
    "\n",
    "for data in datas:\n",
    "    if data['content'][-1].isalnum()==False:\n",
    "        data['content']=data['content'][:-1]\n",
    "    texts.append(data['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FCFew_data_label(data,token_span,ovlp_id):\n",
    "    result_span = torch.zeros(len(token_span),len(token_span))\n",
    "    result_relation = torch.zeros(len(token_span),len(token_span))\n",
    "    overlap_ids=torch.zeros(len(token_span))\n",
    "    index=data['index']\n",
    "    trigger_start=-1\n",
    "    d=data['triggers'][index]\n",
    "    tri_start,tri_end=d[0],d[1]\n",
    "    arg_list=[]\n",
    "    arg_start,arg_end=-1,-1\n",
    "    for i, (offset_start, offset_end) in enumerate(token_span):\n",
    "        if offset_start >= tri_start and offset_end <= tri_end:\n",
    "            if offset_start == tri_start:\n",
    "                trigger_start=i\n",
    "            if offset_end == tri_end:\n",
    "                trigger_end=i\n",
    "                result_span[trigger_start,trigger_end]=2 #触发词\n",
    "                for pos in range(trigger_start,trigger_end+1):\n",
    "                    overlap_ids[pos]=ovlp_id\n",
    "                continue\n",
    "        for value in data['args'].values():\n",
    "            flag=0\n",
    "            for v in value:\n",
    "                if v[0] <= offset_start and v[1] >= offset_end:\n",
    "                    if offset_start == v[0]:\n",
    "                        arg_start=i\n",
    "                        flag=1\n",
    "                        if offset_end == v[1]:\n",
    "                            result_span[i,i]=1\n",
    "                        break\n",
    "                        \n",
    "                    if offset_end == v[1]:\n",
    "                        arg_end=i\n",
    "                        result_span[arg_start,arg_end]=1 #论元\n",
    "                        arg_list.append([arg_start,arg_end])\n",
    "                        flag=1\n",
    "                        break\n",
    "\n",
    "            if flag==1:\n",
    "                break\n",
    "\n",
    "    assert (overlap_ids>=1).sum()!=0,print('error:',data)\n",
    "    assert trigger_start!=-1\n",
    "    for s,e in arg_list:\n",
    "            if result_relation[trigger_start][s]==0 and result_relation[trigger_start][e]==0:\n",
    "                result_relation[trigger_start][s]=1\n",
    "                result_relation[trigger_start][e]=1 #触发词没有重叠\n",
    "\n",
    "            else:\n",
    "                if trigger_start!=trigger_end and result_relation[trigger_start][s]!=0 and result_relation[trigger_start][e]!=0:\n",
    "                    assert 1==0,print(data['id'],arg_list)\n",
    "                    result_relation[trigger_start][s]=2\n",
    "                    result_relation[trigger_start][e]=2 #触发词没有重叠\n",
    "            if result_relation[trigger_end][s]==0 and result_relation[trigger_end][e]==0:\n",
    "                result_relation[trigger_end][s]=1\n",
    "                result_relation[trigger_end][e]=1 #触发词没有重叠\n",
    "\n",
    "            else:\n",
    "                if trigger_start!=trigger_end and result_relation[trigger_end][s]!=0 and result_relation[trigger_end][e]!=0:\n",
    "                    print(data['id'],arg_list)\n",
    "                    assert 1==0\n",
    "                    result_relation[trigger_end][s]=2\n",
    "                    result_relation[trigger_end][e]=2 #触发词没有重叠\n",
    "\n",
    "    return result_span,result_relation,overlap_ids      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sparse(labels):\n",
    "    indices = torch.nonzero(labels)\n",
    "    values = labels[indices[:,0],indices[:,1]]\n",
    "    sparse_labels = torch.sparse_coo_tensor(indices.t(), values, size=labels.shape)\n",
    "    return sparse_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    multilingual_tokenizer_fast=BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')\n",
    "    new_datas=[] #保存添加label的json数据\n",
    "    add_datas=[] #保存修改后的json数据\n",
    "    temp_index=[]\n",
    "    pre_id=None\n",
    "    triggers_count={}\n",
    "    for i,text in enumerate(texts):\n",
    "        j=0\n",
    "        token_results=multilingual_tokenizer_fast.encode_plus(text,return_offsets_mapping=True,add_special_tokens=False)\n",
    "        #判断文本的长度,超过则删除数据\n",
    "        token_len=len(token_results['input_ids'])\n",
    "        if token_len>126:\n",
    "            continue\n",
    "        #判断触发词的重叠次数,超过则删除数据\n",
    "        flag=0\n",
    "\n",
    "\n",
    "        token_span=token_results['offset_mapping']\n",
    "        \n",
    "        if j<len(datas_type) and datas[i]['id']!=datas[j]['id']:\n",
    "            temp_index=[]\n",
    "            j=i\n",
    "        \n",
    "    #     if j>=len(datas_type):\n",
    "    #         j=0\n",
    "    #         continue\n",
    "        data={}\n",
    "        data['id']=datas[i]['id']\n",
    "        data['content']=datas[i]['content']\n",
    "        data['triggers']=datas[i]['triggers']\n",
    "        data['index']=datas[i]['index']\n",
    "        data['args']=datas[i]['args']\n",
    "        \n",
    "        if pre_id==None:\n",
    "            pre_id=data['id']\n",
    "    #         pre_tri=data['triggers']\n",
    "    #         pre_index=data['index']\n",
    "            triggers_count[data['trigger'][data['index']]]=1\n",
    "        else:\n",
    "            if pre_id != data['id']:\n",
    "                for k,v in triggers_count:\n",
    "                    if v<=1:\n",
    "                        \n",
    "                temp_data=deepcopy(new_datas[-1])\n",
    "                size=temp_data['span_label'][2]\n",
    "                temp_data['span_label']=[]\n",
    "                temp_data['span_label'].append(size)\n",
    "    #             size=data['relation_label'][2]\n",
    "                temp_data['relation_label']=[]\n",
    "                temp_data['relation_label'].append(size)\n",
    "                for i,d in enumerate(temp_data['overlap_ids']):\n",
    "                    if d==1:\n",
    "                        temp_data['overlap_ids'][i]=2.0\n",
    "                pre_id = data['id']\n",
    "                new_datas.append(temp_data)\n",
    "        index_count=0\n",
    "        for index in temp_index:\n",
    "            if index==data['triggers'][data['index']]:\n",
    "                index_count=index_count+1\n",
    "        temp_index.append(data['triggers'][data['index']])  \n",
    "        ovlp_id=index_count+1\n",
    "        span_label,relation_label,overlap_ids=FCFew_data_label1(data,token_span,ovlp_id)\n",
    "\n",
    "        #转换成稀疏标签\n",
    "        span_label=label_sparse(span_label)\n",
    "        relation_label=label_sparse(relation_label)\n",
    "\n",
    "        a=[span_label._indices().tolist(),span_label._values().tolist(),span_label.size()]\n",
    "    #     datas[i]['span_label']=[{k:v} for k,v in zip(['indices','values','size'],a)]\n",
    "        data['span_label']=a\n",
    "        b=[relation_label._indices().tolist(),relation_label._values().tolist(),relation_label.size()]\n",
    "        data['relation_label']=b\n",
    "\n",
    "        data['overlap_ids']=overlap_ids.tolist()\n",
    "\n",
    "\n",
    "        #添加修改contexnt后的json数据\n",
    "        new_datas.append(data)\n",
    "\n",
    "    with open('/kaggle/working/new_train_labels.json','a') as out_file:\n",
    "        for data in new_datas:\n",
    "            json.dump(data,out_file,ensure_ascii=False)\n",
    "            out_file.write('\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
